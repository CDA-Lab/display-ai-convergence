{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1609557301786,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "LmunuuEa7rPN"
   },
   "outputs": [],
   "source": [
    "project_name = 'wgan_face_generation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN modified of DCGAN in:\n",
    "1. remove sigmoid in the last layer of discriminator(classification -> regression)                                    \n",
    "2. no log Loss (Wasserstein distance)\n",
    "\n",
    "3. clip param norm to c (Wasserstein distance and Lipschitz continuity)\n",
    "\n",
    "4. No momentum-based optimizer, use RMSPropï¼ŒSGD instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1609557328287,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "p2VLPRoq7rPP",
    "outputId": "63287108-caf3-431d-f1f0-212ca043cfba"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = '../input/anime-faces'\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1609556268271,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "hg9EwokF7rPQ",
    "outputId": "09309fa0-c3f0-4ca5-8c5a-b0c5f24e8f9d"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(DATA_DIR+'/data')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDRRG5907rPQ"
   },
   "source": [
    "Let's load this dataset using the `ImageFolder` class from `torchvision`. We will also resize and crop the images to 64x64 px, and normalize the pixel values with a mean & standard deviation of 0.5 for each channel. This will ensure that pixel values are in the range `(-1, 1)`, which is more  convenient for training the discriminator. We will also create a data loader to load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1453,
     "status": "ok",
     "timestamp": 1609557362329,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "hmRajdDe7rPR"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1609557364848,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "OSlbIxS37rPR"
   },
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1609557366331,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "aLp6NN8B7rPR"
   },
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.CenterCrop(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)]))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n50e7mfZ7rPS"
   },
   "source": [
    "Let's create helper functions to denormalize the image tensors and display some sample images from a training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1609557370273,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "tPVrSXO17rPS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1609557373201,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "lJTY6BMf7rPS"
   },
   "outputs": [],
   "source": [
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1609557373726,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "Jntkukw47rPT"
   },
   "outputs": [],
   "source": [
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for images, _ in dl:\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7657,
     "status": "ok",
     "timestamp": 1609557381629,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "laQtLhl_7rPT",
    "outputId": "370871d8-e051-45ad-c670-8475168c0cd3"
   },
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJWM0w8-7rPW"
   },
   "source": [
    "## Using a GPU\n",
    "\n",
    "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU, if one is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1609557402019,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "fb8ORjt57rPX"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRxpHAp67rPX"
   },
   "source": [
    "Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1609557404702,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "yutNeU5w7rPX",
    "outputId": "2371912b-fbed-4630-adac-a759d78366f6"
   },
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TOp0Zwt7rPY"
   },
   "source": [
    "We can now move our training data loader using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1609557406536,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "ryTEXQuD7rPY"
   },
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoOJIJ937rPY"
   },
   "source": [
    "## Discriminator Network\n",
    "\n",
    "The discriminator takes an image as input, and tries to classify it as \"real\" or \"generated\". In this sense, it's like any other neural network. We'll use a convolutional neural networks (CNN) which outputs a single number output for every image. We'll use stride of 2 to progressively reduce the size of the output feature map.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1609557408086,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "htcCHpje7rPZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1609557409513,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "6nq2pxPJ7rPZ"
   },
   "outputs": [],
   "source": [
    "discriminator = nn.Sequential(\n",
    "    # in: 3 x 64 x 64\n",
    "\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # out: 1 x 1 x 1\n",
    "    \n",
    "    # Modification 1: remove sigmoid\n",
    "    # nn.Sigmoid()\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzjyaUaC7rPZ"
   },
   "source": [
    "Note that we're using the Leaky ReLU activation for the discriminator.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=\"420\">\n",
    "\n",
    "\n",
    ">  Different from the regular ReLU function, Leaky ReLU allows the pass of a small gradient signal for negative values. As a result, it makes the gradients from the discriminator flows stronger into the generator. Instead of passing a gradient (slope) of 0 in the back-prop pass, it passes a small negative gradient.  - [Source](https://sthalles.github.io/advanced_gans/)\n",
    "\n",
    "Just like any other binary classification model, the output of the discriminator is a single number between 0 and 1, which can be interpreted as the probability of the input image being real i.e. picked from the original dataset.\n",
    "\n",
    "Let's move the discriminator model to the chosen device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1609557411029,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "tFT_FGnN7rPZ"
   },
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HdpVR1n7rPa"
   },
   "source": [
    "## Generator Network\n",
    "\n",
    "The input to the generator is typically a vector or a matrix of random numbers (referred to as a latent tensor) which is used as a seed for generating an image. The generator will convert a latent tensor of shape `(128, 1, 1)` into an image tensor of shape `3 x 28 x 28`. To achive this, we'll use the `ConvTranspose2d` layer from PyTorch, which is performs to as a *transposed convolution* (also referred to as a *deconvolution*). [Learn more](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)\n",
    "\n",
    "![](https://i.imgur.com/DRvK546.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1609557412746,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "496i2VzW7rPa"
   },
   "outputs": [],
   "source": [
    "latent_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1609557414003,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "XuDQRN827rPc"
   },
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.Tanh()\n",
    "    # out: 3 x 64 x 64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T4IiCB97rPc"
   },
   "source": [
    "We use the TanH activation function for the output layer of the generator.\n",
    "\n",
    "<img src=\"https://nic.schraudolph.org/teach/NNcourse/figs/tanh.gif\" width=\"420\" >\n",
    "\n",
    "> \"The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling.\" - [Source](https://stackoverflow.com/questions/41489907/generative-adversarial-networks-tanh)\n",
    "\n",
    "\n",
    "Note that since the outputs of the TanH activation lie in the range `[-1,1]`, we have applied the similar transformation to the images in the training dataset. Let's generate some outputs using the generator and view them as images by transforming and denormalizing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    # weight_initialization: important for wgan\n",
    "    class_name=m.__class__.__name__\n",
    "    if class_name.find('Conv')!=-1:\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "    elif class_name.find('Norm')!=-1:\n",
    "        m.weight.data.normal_(1.0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.apply(weight_init)\n",
    "generator.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3326,
     "status": "ok",
     "timestamp": 1609557418762,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "9_aHHWA07rPd",
    "outputId": "1a829c45-9b1c-407e-da99-10be6b8ff848"
   },
   "outputs": [],
   "source": [
    "xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\n",
    "fake_images = generator(xb)\n",
    "print(fake_images.shape)\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6CwMocE7rPd"
   },
   "source": [
    "As one might expect, the output from the generator is basically random noise, since we haven't trained it yet. \n",
    "\n",
    "Let's move the generator to the chosen device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1609557420110,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "WhsPkkCP7rPe"
   },
   "outputs": [],
   "source": [
    "generator = to_device(generator, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHzgNaR57rPe"
   },
   "source": [
    "## Discriminator Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1609557421710,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "xz3jTbye7rPe"
   },
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_d):\n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # Pass real images through discriminator\n",
    "    real_preds = discriminator(real_images)\n",
    "    #modification: remove binary cross entropy\n",
    "        #real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "        #real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_loss = -torch.mean(real_preds)\n",
    "        \n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    # Pass fake images through discriminator\n",
    "    #modification: remove binary cross entropy\n",
    "        #fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "    fake_preds = discriminator(fake_images)\n",
    "        #fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_loss = torch.mean(fake_preds)\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()\n",
    "    opt_d.step()\n",
    "    return loss.item(), real_loss.item(), fake_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKyS1SKA7rPe"
   },
   "source": [
    "Here are the steps involved in training the discriminator.\n",
    "\n",
    "- We expect the discriminator to output 1 if the image was picked from the real MNIST dataset, and 0 if it was generated using the generator network.Â \n",
    "\n",
    "- We first pass a batch of real images, and compute the loss, setting the target labels to 1.Â \n",
    "\n",
    "- Then we pass a batch of fake images (generated using the generator) pass them into the discriminator, and compute the loss, setting the target labels to 0.Â \n",
    "\n",
    "- Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator.\n",
    "\n",
    "It's important to note that we don't change the weights of the generator model while training the discriminator (`opt_d` only affects the `discriminator.parameters()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt_tkA547rPf"
   },
   "source": [
    "## Generator Training\n",
    "\n",
    "Since the outputs of the generator are images, it's not obvious how we can train the generator. This is where we employ a rather elegant trick, which is to use the discriminator as a part of the loss function. Here's how it works:\n",
    "\n",
    "- We generate a batch of images using the generator, pass the into the discriminator.\n",
    "\n",
    "- We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.Â \n",
    "\n",
    "- We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator.\n",
    "\n",
    "Here's what this looks like in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1609557424464,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "5lmjNkZE7rPf"
   },
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "    \n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "    \n",
    "    # Try to fool the discriminator\n",
    "    preds = discriminator(fake_images)\n",
    "    #modificationL remove binary cross entropy\n",
    "        #targets = torch.ones(batch_size, 1, device=device)\n",
    "    loss = -torch.mean(preds)\n",
    "    \n",
    "    # Update generator weights\n",
    "    loss.backward()\n",
    "    opt_g.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo00Pcmu7rPg"
   },
   "source": [
    "Let's create a directory where we can save intermediate outputs from the generator to visually inspect the progress of the model. We'll also create a helper function to export the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1609557425981,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "kiQyhcI37rPg"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1609557427010,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "OhMPV5bk7rPg"
   },
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1609557428102,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "LiCEUuDE7rPg"
   },
   "outputs": [],
   "source": [
    "def save_samples(index, latent_tensors, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQNysyNY7rPg"
   },
   "source": [
    "We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1609557430507,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "PQN4Hvkm7rPg"
   },
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2558,
     "status": "ok",
     "timestamp": 1609557433582,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "VLZaqP0T7rPh",
    "outputId": "5da97c2c-721a-433c-d58b-405a247200f2"
   },
   "outputs": [],
   "source": [
    "save_samples(0, fixed_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_uc0l7r7rPh"
   },
   "source": [
    "## Full Training Loop\n",
    "\n",
    "Let's define a `fit` function to train the discriminator and generator in tandem for each batch of training data. We'll use the Adam optimizer with some custom parameters (betas) that are known to work well for GANs. We will also save some sample generated images at regular intervals for inspection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1609557447799,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "bFfiZRa37rPh"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1609557449856,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "XHsBi5hW7rPh"
   },
   "outputs": [],
   "source": [
    "def fit(epochs, lr, start_idx=1):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Losses & scores\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "    \n",
    "    # Create optimizers\n",
    "    opt_d = torch.optim.RMSprop(discriminator.parameters(), lr=lr)\n",
    "    opt_g = torch.optim.RMSprop(generator.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for real_images, _ in tqdm(train_dl):\n",
    "            # Train discriminator\n",
    "            # modification: clip param for discriminator\n",
    "            for parm in discriminator.parameters():\n",
    "                parm.data.clamp_(-clamp_num, clamp_num)\n",
    "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "            # Train generator\n",
    "            loss_g = train_generator(opt_g)\n",
    "            \n",
    "        # Record losses & scores\n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        # Log losses & scores (last batch)\n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "    \n",
    "        # Save generated images\n",
    "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "    \n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxw7tdof7rPh"
   },
   "source": [
    "We are now ready to train the model. Try different learning rates to see if you can maintain the fine balance between the training the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1609557452868,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "KCO9Xz8a7rPi"
   },
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "epochs = 200\n",
    "clamp_num=0.01# WGAN clip gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3370548,
     "status": "ok",
     "timestamp": 1609560825564,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "QukXeGTw7rPi",
    "outputId": "91aee3dd-97b0-4c14-9efc-ecbb1d417458"
   },
   "outputs": [],
   "source": [
    "history = fit(epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1609561100843,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "Uzxo0l047rPi"
   },
   "outputs": [],
   "source": [
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELs5OAbH7rPj"
   },
   "source": [
    "Now that we have trained the models, we can save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1609561103993,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "b55q3sQR7rPj"
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoints \n",
    "torch.save(generator.state_dict(), 'G.pth')\n",
    "torch.save(discriminator.state_dict(), 'D.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctJgUSgt7rPj"
   },
   "source": [
    "Here's how the generated images look, after the 1st, 5th and 10th epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1609561105630,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "b033aUC07rPj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2073,
     "status": "ok",
     "timestamp": 1609561108112,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "L4Hpc2Wm7rPj",
    "outputId": "f75cb15d-d6ed-4241-8dbe-97c0ceceb287"
   },
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dp3Y77VU7rPk"
   },
   "source": [
    "We can visualize the training process by combining the sample images generated after each epoch into a video using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2138,
     "status": "ok",
     "timestamp": 1609561127532,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "bfABlRRl7rPk"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "vid_fname = 'gans_training.avi'\n",
    "\n",
    "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\n",
    "files.sort()\n",
    "\n",
    "out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
    "[out.write(cv2.imread(fname)) for fname in files]\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9OFv0oj7rPk"
   },
   "source": [
    "Here's what it looks like:\n",
    "\n",
    "![]()\n",
    "\n",
    "\n",
    "We can also visualize how the loss changes over time. Visualizing \n",
    "losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1066,
     "status": "ok",
     "timestamp": 1609561131690,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "lLdS3gb87rPk",
    "outputId": "3a3319bb-f866-471d-e6e1-c5de7533e48d"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses_d, '-')\n",
    "plt.plot(losses_g, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1609561133661,
     "user": {
      "displayName": "Aakash N S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIWFHtan62vtW1gz2Bv2bxL3rppefcadxzEVxRKQ=s64",
      "userId": "03254185060287524023"
     },
     "user_tz": -330
    },
    "id": "McZ165r67rPl",
    "outputId": "c512ed6a-acbc-43f6-b6a7-3f562864c887"
   },
   "outputs": [],
   "source": [
    "plt.plot(real_scores, '-')\n",
    "plt.plot(fake_scores, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.title('Scores');"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}